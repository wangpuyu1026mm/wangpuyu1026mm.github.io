<h2>Selected Publications Last 5 Years</h2><p>
  
[1] <strong>Puyu Wang</strong>,YunwenLei, Di Wang, YimingYingandDing-XuanZhou.“GeneralizationGuarantees of Gradient Descent for Shallow Neural Networks.” <strong>Neural Computation</strong>.  ,37(2):344-402, 2025.

[2]<strong>Puyu Wang</strong>,  Yunwen Lei, Yiming Ying and Hai Zhang. “Differentially Private SGDwith Non-smooth Losses.”  <strong>Applied and Computational Harmonic Analysis</strong>.  , 56:306-336,2022. 

[3]<strong>Puyu Wang</strong>,  Yunwen Lei, Yiming Ying and Ding-Xuan Zhou.“Stability and Generalization for Markov Chain Stochastic Gradient Methods.” <strong>In 36th Advances in NeuralInformation Processing Systems (NeurIPS) </strong>,2022

[4]<strong>Puyu Wang</strong>,  Liang Wu and Yunwen Lei. “Stability and Generalization for RandomizedCoordinate Descent.” <strong>In 30th International Joint Conference on Artificial Intelligence(IJCAI)</strong>, 2021. 
</p>

<h2>Preprint</h2><p>

 [1] Yunwen Lei,<strong> Puyu Wang</strong>, Yiming Ying and Ding-Xuan Zhou. “Optimization andGeneralization of Gradient Descent for Shallow ReLU Networks with PolylogarithmicWidth.” <strong>Journal of Machine Learning Research, Accepted with minor revision</strong>.
 
 [2] Junyu Zhou, Shuo Huang, Han Feng, <strong>Puyu Wang∗</strong> and Ding-Xuan Zhou. “ Finegrained Analysis of Non-parametric Estimation for Pairwise Learning.” <strong>Under Revisionat IEEE Transactions on Neural Networks and Learning Systems</strong>. (∗correspondingauthor)
 
 [3] <strong>Puyu Wang</strong>, Junyu Zhou, Yunwen Lei, Jun Fan and Yiming Ying.“ Optimal Rates forGradient Methods with Two-Layer ReLU Neural Networks.” <strong>Submitted</strong>
 
 [4] <strong>Puyu Wang</strong>, Junyu Zhou, Yunwen Lei, Marius Kloft and Yiming Ying.“ Generalizationin Deep Neural Networks: Minimax Rates for Gradient Methods.” <strong>Submitted</strong>
 
 [5] Junyu Zhou, <strong>Puyu Wang</strong>, Yunwen Lei, Yiming Ying and Ding-Xuan Zhou.“ OptimalRates for Generalization of Gradient Descent Methods with Deep ReLU Networks.”<strong>Submitted</strong>
 
 [6] Junyu Zhou, <strong>Puyu Wang</strong> and Ding-Xuan Zhou.“ Generalization Analysis with DeepReLU Networks for Metric and Similarity Learning.” <strong>Submitted</strong>
 
 [7] Zhongjie Shi, <strong>Puyu Wang</strong>, Chenyang Zhang and Yuan Cao. “Towards UnderstandingGeneralization in DP-GD: A Case Study in Training Two-Layer CNNs.” <strong>Submitted</strong>
 
 [8] Liang Wu, <strong>Puyu Wang</strong> and Yunwen Lei. “Stability Analysis of Randomized BlockCoordinate Descent with General Sampling.” <strong>Submitted</strong>
 
 [9] Marcio Montero, Weichen Li, <strong>Puyu Wang</strong>, MariusKloftandSophie Fellenz. “LandmarGuided Policy Optimization for Multi-Objective Language Model Selection.”<strong> Submitted</strong>
 
 [10] Weichen Li, Waleed Mustafa, Marcio Monteiro, <strong>Puyu Wang</strong>, Marius Kloft and SophieFellenz. “Train Once, Align Anytime: Inference-Time Preference Alignment for OfflineMulti-Objective Reinforcement Learning” <strong>Submitted</strong>
 
 [11] Philipp Liznerski, Saurabh Varshneya, Ece Calikus, <strong>Puyu Wang</strong>, Alexander Bartscher,Sebastian Josef Vollmer, Sophie Fellenz and Marius Kloft. “Reimagining Anomalies:What If Anomalies Were Normal?” <strong>Submitted</strong>
 
 </p>

 <h2>Refereed Journal Papers</h2><p>

 [1] <strong>Puyu Wang</strong>,YunwenLei, DiWang, YimingYingandDing-XuanZhou.“GeneralizationGuarantees of Gradient Descent for Shallow Neural Networks.” <strong>Neural Computation</strong>,37(2):344-402, 2025.
 
 [2] <strong>Puyu Wang</strong>, Yunwen Lei, Yiming Ying and Ding-Xuan Zhou.“Differentially PrivateStochastic Gradient Descent with Low-Noise.”<strong>Neurocomputing</strong>, 585:127557, 2024.
 
 [3] Theophilus Asenso, <strong>Puyu Wang</strong> and Hai Zhang. “Pliable Lasso for the Support VectorMachine.” <strong>Communications in Statistics–Simulation and Computation</strong>, 53(2): 786-798,2024.
 
 [4] <strong>Puyu Wang</strong>, Yunwen Lei, Yiming Ying and Hai Zhang. “Differentially Private SGDwith Non-smooth Losses.” <strong>Applied and Computational Harmonic Analysis</strong>, 56:306-336,2022.
 
 [5] <strong>Puyu Wang</strong>, Zhenhuan Yang, Yunwen Lei, Yiming Ying and Hai Zhang. “DifferentiallyPrivate Empirical Risk Minimization for AUC Maximization.” <strong>Neurocomputing</strong>, 461:419-437, 2021.
 
 [6] <strong>Puyu Wang</strong> and Hai Zhang. “Differential Privacy for Sparse Classification Learning.”<strong>Neurocomputing</strong>, 375: 91-101, 2020.
 
 [7] <strong>Puyu Wang</strong> and HaiZhang. “DistributedLogistic Regression with Differential Privacy.”<strong>SCIENTIA SINICA Informationis (Chinese version)</strong>, 50: 1511-1528, 2020.
 
 [8] <strong>Puyu Wang</strong>, Hai Zhang and Yong Liang. “Model Selection with Distributed SCADPenalty.” <strong>Journal of Applied Statistics</strong>, 45(11): 1938-1955, 2018.
 
 [9] Hai Zhang, <strong>Puyu Wang</strong>, Qing Dong and Pu Wang. “Sparse Bayesian Linear Regressionusing Generalized Normal Priors.” <strong>International Journal of Wavelets Multiresolutionand Information Processing</strong>, 15(3): 1750021, 2017. </p>

 <h2>Refereed Conference Papers</h2><p>
 [1] <strong>Puyu Wang</strong>, Yunwen Lei, Marius Kloft and Yiming Ying.“Optimal Utility Bounds forDifferentially Private Gradient Descent in Three-Layer Neural Networks.” <strong>In 12th IEEEInternational Conference on Data Science and Advanced Analytics (DSAA)</strong>, 2025.
   
 [2] Weichen Li, Waleed Mustafa, <strong>Puyu Wang</strong>, Marius Kloft and Sophie Fellenz. “InferenceTime Preference-Aligned Diffusion Planning for Safe Offline Reinforcement Learning.”<strong>ECML PKDD Workshop on Towards Hybrid Human-Machine Learning and Decision Making</strong>, 2025.

 [3] Waleed Mustafa, Philipp Liznerski, Antoine Ledent, Dennis Wagner, <strong>Puyu Wang</strong> andMarius Kloft." Non-vacuous Generalization Bounds for Adversarial Risk in StochasticNeural Networks." <strong>In International Conference on Artificial Intelligence and Statistics(AISTATS)</strong>, 2024.
 
 [4] Waleed Mustafa, Philipp Liznerski, Dennis Wagner, <strong>Puyu Wang</strong>, Marius Kloft. “Nonvacuous PAC-Bayes bounds for models under adversarial corruptions.” <strong>In PAC-Bayes Meets Interactive Learning Workshop at International Conference on Machine Learning(ICML workshop)</strong>, 2023.
 
 [5] <strong>Puyu Wang</strong>, Yunwen Lei, Yiming Ying and Ding-Xuan Zhou.“Stability and Generaliza-tion for Markov Chain Stochastic Gradient Methods.” <strong>In 36th Advances in NeuralInformation Processing Systems (NeurIPS)</strong>, 2022.
 
 [6] Zhenhuan Yang, Yunwen Lei, <strong>Puyu Wang</strong>, Tianbao Yang and Yiming Ying. “Simple Stochastic and Online Gradient Descent Algorithms for Pairwise Learning.” <strong>In 35th Advances in Neural Information Processing Systems (NeurIPS)</strong>, 2021.
 
 [7] <strong>Puyu Wang</strong>, Liang Wu and Yunwen Lei. “Stability and Generalization for Randomized Coordinate Descent.” <strong>In 30th International Joint Conference on Artificial Intelligence(IJCAI)</strong>, 2021.
 
 [8] Peishen Shi, <strong>Puyu Wang</strong> andHaiZhang. “DistributedLogistic Regression for Separated Massive Data.” <strong>In 7th CCF conference on Big Data, 2019. (Best Student Paper Award)</strong>
 
