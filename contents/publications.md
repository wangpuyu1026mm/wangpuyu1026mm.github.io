
<h2>Preprint</h2><p>
 
 [1] Junyu Zhou, Shuo Huang, Han Feng, <strong>Puyu Wang<sup>*</sup></strong> and Ding-Xuan Zhou.  [Fine-grained Analysis of Non-parametric Estimation for Pairwise Learning.](https://arxiv.org/pdf/2305.19640)  *Under Revision at IEEE Transactions on Neural Networks and Learning Systems*. (corresponding author)
 
 [2] Junyu Zhou, <strong>Puyu Wang</strong> and Ding-Xuan Zhou. [Generalization Analysis with Deep ReLU Networks for Metric and Similarity Learning.](https://arxiv.org/pdf/2405.06415?)  *Submitted*
 
 [3] Philipp Liznerski, Saurabh Varshneya, Ece Calikus, <strong>Puyu Wang</strong>, Alexander Bartscher,Sebastian Josef Vollmer, Sophie Fellenz and Marius Kloft.  Reimagining Anomalies: What If Anomalies Were Normal?  *Submitted*
 
 </p>

 <h2>Refereed Journal Papers</h2><p>

 [1] <strong>Puyu Wang</strong>,Yunwen Lei, Di Wang, Yiming Ying and Ding-XuanZhou. [GeneralizationGuarantees of Gradient Descent for Shallow Neural Networks.](https://direct.mit.edu/neco/article-abstract/37/2/344/125265/Generalization-Guarantees-of-Gradient-Descent-for?redirectedFrom=fulltext) *Neural Computation*,37(2):344-402, 2025.
 
 [2] <strong>Puyu Wang</strong>, Yunwen Lei, Yiming Ying and Ding-Xuan Zhou. [Differentially Private Stochastic Gradient Descent with Low-Noise.](https://www.sciencedirect.com/science/article/pii/S092523122400328X) *Neurocomputing*, 585:127557, 2024.
 
 [3] Theophilus Asenso, <strong>Puyu Wang</strong> and Hai Zhang. [Pliable Lasso for the Support VectorMachine.](https://www.tandfonline.com/doi/abs/10.1080/03610918.2022.2032160) *Communications in Statisticsâ€“Simulation and Computation*, 53(2): 786-798,2024.
 
 [4] <strong>Puyu Wang</strong>, Yunwen Lei, Yiming Ying and Hai Zhang. [Differentially Private SGD with Non-smooth Losses.](https://arxiv.org/pdf/2101.08925) *Applied and Computational Harmonic Analysis*, 56:306-336,2022.
 
 [5] <strong>Puyu Wang</strong>, Zhenhuan Yang, Yunwen Lei, Yiming Ying and Hai Zhang. [DifferentiallyPrivate Empirical Risk Minimization for AUC Maximization.](https://par.nsf.gov/servlets/purl/10274889) *Neurocomputing*, 461:419-437, 2021.
 
 [6] <strong>Puyu Wang</strong> and Hai Zhang. [Differential Privacy for Sparse Classification Learning.](https://www.sciencedirect.com/science/article/abs/pii/S0925231219312822) *Neurocomputing*, 375: 91-101, 2020.
 
 [7] <strong>Puyu Wang</strong> and Hai Zhang. DistributedLogistic Regression with Differential Privacy. *SCIENTIA SINICA Informationis (Chinese version)*, 50: 1511-1528, 2020.
 
 [8] <strong>Puyu Wang</strong>, Hai Zhang and Yong Liang. [Model Selection with Distributed SCAD Penalty.](https://www.tandfonline.com/doi/abs/10.1080/02664763.2017.1401052) *Journal of Applied Statistics*, 45(11): 1938-1955, 2018.
 
 [9] Hai Zhang, <strong>Puyu Wang</strong>, Qing Dong and Pu Wang. [Sparse Bayesian Linear Regressionusing Generalized Normal Priors.](https://www.worldscientific.com/doi/abs/10.1142/S0219691317500217) *International Journal of Wavelets Multiresolutionand Information Processing*, 15(3): 1750021, 2017. </p>

 <h2>Refereed Conference Papers</h2><p>
   
 [1] <strong>Puyu Wang</strong>, Yunwen Lei, Marius Kloft and Yiming Ying.  Optimal Utility Bounds forDifferentially Private Gradient Descent in Three-Layer Neural Networks.  *IEEE DSAA*, 2025.
   
 [2] Weichen Li, Waleed Mustafa, <strong>Puyu Wang</strong>, Marius Kloft and Sophie Fellenz. Inference Time Preference-Aligned Diffusion Planning for Safe Offline Reinforcement Learning. *ECML PKDD Workshop on Towards Hybrid Human-Machine Learning and Decision Making*, 2025.

 [3] Waleed Mustafa, Philipp Liznerski, Antoine Ledent, Dennis Wagner, <strong>Puyu Wang</strong> and Marius Kloft. [Non-vacuous Generalization Bounds for Adversarial Risk in Stochastic Neural Networks.](https://proceedings.mlr.press/v238/mustafa24a.html) *AISTATS*, 2024.
 
 [4] Waleed Mustafa, Philipp Liznerski, Dennis Wagner, <strong>Puyu Wang</strong>, Marius Kloft.  Non vacuous PAC-Bayes bounds for models under adversarial corruptions.  *ICML workshop*, 2023.
 
 [5] <strong>Puyu Wang</strong>, Yunwen Lei, Yiming Ying and Ding-Xuan Zhou. [Stability and Generaliza-tion for Markov Chain Stochastic Gradient Methods.](https://proceedings.neurips.cc/paper_files/paper/2022/file/f61538f83b0f19f9306d9d801c15f41c-Paper-Conference.pdf) *NeurIPS*, 2022.
 
 [6] Zhenhuan Yang, Yunwen Lei, <strong>Puyu Wang</strong>, Tianbao Yang and Yiming Ying. [Simple Stochastic and Online Gradient Descent Algorithms for Pairwise Learning.](https://proceedings.neurips.cc/paper/2021/file/a87d27f712df362cd22c7a8ef823e987-Paper.pdf) *NeurIPS*, 2021.
 
 [7] <strong>Puyu Wang</strong>, Liang Wu and Yunwen Lei. [Stability and Generalization for Randomized Coordinate Descent.](https://arxiv.org/pdf/2108.07414) *IJCAI*, 2021.
 
 
